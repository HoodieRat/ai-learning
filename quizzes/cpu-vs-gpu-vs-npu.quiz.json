{
  "title": "Hardware: CPU vs GPU vs NPU — Quiz",
  "passingScore": 70,
  "questions": [
    {
      "type": "multiple-choice",
      "question": "Which hardware is typically best for highly parallel matrix math in modern ML inference?",
      "options": [
        "GPU",
        "CPU",
        "Hard drive",
        "Network card"
      ],
      "answer": "GPU",
      "explanation": "GPUs are designed for parallel workloads and often provide high throughput for tensor/matrix ops."
    },
    {
      "type": "multiple-choice",
      "question": "What is VRAM primarily used for during GPU inference?",
      "options": [
        "Storing model weights and activations on the GPU",
        "Storing files long-term",
        "Increasing internet speed",
        "Replacing the CPU"
      ],
      "answer": "Storing model weights and activations on the GPU",
      "explanation": "VRAM is the GPU’s fast memory used to hold data needed for compute."
    },
    {
      "type": "true-false",
      "question": "If your model does not fit in VRAM, it may fall back to slower paths (or fail to load).",
      "answer": true,
      "explanation": "Memory constraints can cause swapping/offloading and major slowdowns, or prevent loading."
    },
    {
      "type": "multi-select",
      "question": "Which can be bottlenecks for local AI performance?",
      "options": [
        "Memory bandwidth",
        "Thermal limits",
        "Disk capacity (for inference speed)",
        "PCIe transfer overhead"
      ],
      "answer": [
        "Memory bandwidth",
        "Thermal limits",
        "PCIe transfer overhead"
      ],
      "explanation": "Bandwidth, thermals, and host↔device transfers matter; disk capacity usually affects storage, not compute throughput."
    },
    {
      "type": "multiple-choice",
      "question": "NPUs are generally optimized for:",
      "options": [
        "Efficient inference for certain neural workloads",
        "High-end gaming graphics",
        "Long-term archival storage",
        "Running web servers"
      ],
      "answer": "Efficient inference for certain neural workloads",
      "explanation": "NPUs target efficient on-device inference for specific operator sets and models."
    },
    {
      "type": "true-false",
      "question": "More FLOPS always means faster real-world inference.",
      "answer": false,
      "explanation": "Real performance also depends on memory bandwidth, kernels, batch size, and overheads."
    }
  ]
}
