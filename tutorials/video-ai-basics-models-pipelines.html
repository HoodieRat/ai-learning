<article class="tutorial">
  <h1>Video AI: Models &amp; Pipelines</h1>
  <p class="lede">Video generation is like image generation plus time. Stability problems (flicker, identity drift) are the default unless you design for consistency.</p>

  <h2>Prereqs</h2>
  <ul>
    <li><a href="?t=image-ai-models-and-workflows">Image AI: Models &amp; Workflows</a></li>
  </ul>

  <h2>Pipeline building blocks</h2>
  <ul>
    <li>Storyboard (what each shot should do)</li>
    <li>Keyframes / reference images (consistency anchor)</li>
    <li>Clip generation and refinement</li>
  </ul>

  <h2>Worked example: 3-shot plan</h2>
  <ol>
    <li>Write three shots: subject, action, camera move, background.</li>
    <li>Add consistency notes: outfit, colors, lighting to keep across shots.</li>
    <li>Pick or generate a keyframe per shot to anchor identity.</li>
    <li>Define pass/fail: if identity or outfit drifts, the shot fails.</li>
  </ol>

  <h2>Mini build: preflight</h2>
  <ol>
    <li>Create a 10–15s storyboard with 3 shots.</li>
    <li>Generate keyframes for each; ensure identity matches.</li>
    <li>Decide clip lengths (short) and which constraint to enforce (reference, seed, prompt lock).</li>
    <li>Note what you will check after each clip (identity, flicker, background continuity).</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Plan a 10–15s sequence before generating anything.</p>
    <p><strong>Do:</strong> Write 3 shots with a clear subject, action, and camera description. Keep the same identity constraints across shots.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>You have a storyboard with 3 shots.</li>
      <li>Each shot has a “must keep” consistency note (identity, outfit, background).</li>
      <li>You can tell if a clip failed based on the storyboard alone.</li>
    </ul>
    <p><strong>Verify:</strong> Generate one clip and check it against the storyboard requirements.</p>
  </section>

  <h2>Practice lab: ComfyUI AnimateDiff (local)</h2>
  <ol>
    <li>Install AnimateDiff nodes in ComfyUI; load a text-to-video flow with SDXL base.</li>
    <li>Use the sample storyboard/keyframes in <a href="assets/examples/video/">assets/examples/video/</a> or write your own 3 shots.</li>
    <li>Settings: 512x512, 12–16 fps, 16–24 frames, sampler euler, steps 24, CFG 7, fixed seed per shot; reuse the matching keyframe/control image.</li>
    <li>After each render, check identity drift/flicker; rerun only failing shots.</li>
  </ol>

  <h2>Practice lab: Pika/Runway (SaaS option)</h2>
  <ol>
    <li>Use the same 3-shot storyboard; input the prompt per shot with identity/lighting constraints.</li>
    <li>Export MP4s; trim to 3–4s per shot. Note any vendor-specific settings (motion guidance, face lock).</li>
    <li>Assemble clips and assess against the storyboard; re-run only failing shots.</li>
  </ol>

  <h2>Practice lab: assemble and review</h2>
  <ol>
    <li>Concatenate the 3 clips using the provided list: <code>ffmpeg -f concat -safe 0 -i assets/examples/video/concat.txt -c copy out.mp4</code>.</li>
    <li>Checklist: identity stable, camera coherent, no obvious flicker; if not, re-render the offending shot with stricter guidance/seed reuse.</li>
  </ol>

  <p><strong>Learn more:</strong> <a href="https://github.com/huggingface/diffusers/tree/main/examples/text_to_video">Diffusers text-to-video examples</a> and <a href="https://github.com/comfyanonymous/ComfyUI">ComfyUI</a>; SaaS: <a href="https://pika.art/">Pika</a>, <a href="https://runwayml.com/">Runway</a>.</p>

  <h2>Assets you can reuse</h2>
  <ul>
    <li><a href="assets/examples/video/storyboard.txt">Storyboard (3 shots)</a></li>
    <li><a href="assets/examples/video/keyframes.txt">Keyframe prompts</a> and <a href="assets/examples/video/keyframe-prompts.csv">CSV seeds</a></li>
    <li><a href="assets/examples/video/concat.txt">FFmpeg concat list</a></li>
    <li><a href="assets/examples/video/README.md">How to run</a></li>
  </ul>

  <h2>Start → Run → Verify checklist</h2>
  <ul>
    <li>Start: storyboard + keyframes chosen; seed per shot fixed.</li>
    <li>Run: render 3 clips with consistent seed/keyframe; assemble via ffmpeg.</li>
    <li>Verify: identity and background stable across shots; flicker minimal; rerender only failing shots.</li>
  </ul>

  <h2>Recap</h2>
  <ul>
    <li>Storyboards and keyframes are the anchors for identity.</li>
    <li>Keep clips short; enforce one main constraint per shot.</li>
    <li>Define pass/fail visually before you generate.</li>
  </ul>

  <h2>Next</h2>
  <p><a href="?t=video-ai-hands-on-storyboard-to-clips">Video AI: Hands-on Storyboard → Clips</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=image-ai-hands-on-controlnet-and-inpainting">Image AI: Hands-on Control + Inpainting</a></li>
    <li><a href="?t=audio-ai-tts-stt-basics">Audio AI: TTS &amp; STT Basics</a></li>
  </ul>
</article>
