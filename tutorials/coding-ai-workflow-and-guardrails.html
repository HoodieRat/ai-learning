<article class="tutorial">
  <h1>Coding AI: Workflow &amp; Guardrails</h1>
  <p class="lede">Coding with AI works best when you treat it as a collaborator: give it context, constrain the task, and verify outputs. This tutorial gives a repeatable loop you can use in any editor.</p>

  <h2>Prereqs</h2>
  <ul>
    <li><a href="?t=prompting-structure-quickstart">Prompting: Structure Quickstart</a></li>
    <li><a href="?t=evaluation-basics-metrics-vs-judges">Evaluation: Metrics vs Judges</a></li>
  </ul>

  <h2>The loop</h2>
  <ol>
    <li><strong>Define:</strong> input/output, constraints, and non-goals.</li>
    <li><strong>Generate:</strong> ask for a small change, not a rewrite.</li>
    <li><strong>Verify:</strong> run tests, read diffs, and check edge cases.</li>
    <li><strong>Iterate:</strong> refine prompts and add assertions.</li>
  </ol>

  <h2>Worked example: safe small change</h2>
  <ol>
    <li>Task: add input validation to a handler.</li>
    <li>Prompt: provide the existing function, state accepted inputs, reject rules, and the tests you expect.</li>
    <li>Generation: ask for just the validation and one new test.</li>
    <li>Verification: run the test, read the diff, and ensure no unrelated code moved.</li>
  </ol>

  <h2>Mini build: your repeatable loop</h2>
  <ol>
    <li>Pick a tiny function you can test (parse date, sanitize input).</li>
    <li>Write the input/output and 2 constraints (non-goals included).</li>
    <li>Ask for code + tests; keep the change small.</li>
    <li>Run tests, inspect diff, and add one more edge test from your own brain.</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Use AI to implement a small feature with verification.</p>
    <p><strong>Do:</strong> Pick a tiny function you can test (e.g., parse a date, sanitize input). Ask for an implementation + tests.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>You have at least 5 tests, including 2 edge cases.</li>
      <li>You can explain how the tests prove correctness.</li>
      <li>You made at least one correction based on test failures.</li>
    </ul>
    <p><strong>Verify:</strong> Run the tests twice: once before changes (failing) and once after (passing).</p>
  </section>

  <h2>Practice lab: small diff + tests</h2>
  <ol>
    <li>Clone a small repo (or your own) and run tests once: <code>pytest</code> or <code>npm test</code>.</li>
    <li>Prompt your model: “Add input validation to <code>process_item</code>; reject empty ids; add one failing test that proves the new behavior. Do not refactor other code.”</li>
    <li>Apply the patch; rerun tests; ensure only the intended files changed.</li>
    <li>Log: diff summary, test results, and any manual assertions you added.</li>
  </ol>

  <h2>Assets</h2>
  <ul>
    <li><a href="assets/examples/coding/test-plan.md">Test plan template</a></li>
  </ul>

  <h2>Start → Run → Verify checklist</h2>
  <ul>
    <li>Start: repo cloned; baseline tests run; constraints written (inputs, outputs, non-goals).</li>
    <li>Run: generate minimal diff + tests; apply; rerun tests.</li>
    <li>Verify: review diff; ensure only intended files changed; log test results and one added edge case.</li>
  </ul>

  <h2>Practice lab: Terraform guardrails</h2>
  <ol>
    <li>Provide the <code>main.tf</code> snippet and constraints (no new resources, only tag updates, no IAM changes).</li>
    <li>Ask for the minimal change and a <code>terraform plan</code> explanation in 3 bullets.</li>
    <li>Run <code>terraform plan</code>; compare with the model’s plan; if drift, fix manually and re-run.</li>
  </ol>

  <h2>Practice lab: log redaction check</h2>
  <ol>
    <li>Give the model a log excerpt containing fake secrets (e.g., <code>AKIA...</code>, emails).</li>
    <li>Prompt: “Redact secrets with [REDACTED]; output JSON with original_line_no and redacted_line.”</li>
    <li>Add an assertion (script or test) that fails if patterns like <code>AKIA</code> remain.</li>
  </ol>

  <p><strong>Learn more:</strong> <a href="https://learn.microsoft.com/en-us/copilot/overview">GitHub Copilot overview</a> for constrained coding patterns.</p>

  <h2>Recap</h2>
  <ul>
    <li>Keep tasks small and explicit: inputs, outputs, constraints, non-goals.</li>
    <li>Always verify with tests and diff review; never trust first output.</li>
    <li>Add your own edge cases; AI suggestions are not exhaustive.</li>
  </ul>

  <h2>Next</h2>
  <p><a href="?t=coding-ai-hands-on-refactor-with-tests">Coding AI: Hands-on Refactor with Tests</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=agentic-flows-basics-what-agents-are">Agentic Flows: What Agents Are</a></li>
    <li><a href="?t=safety-ethics-practical-ai-use">Safety &amp; Ethics: Practical AI Use</a></li>
  </ul>
</article>
