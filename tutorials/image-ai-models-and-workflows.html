<article class="tutorial">
  <h1>Image AI: Models &amp; Workflows</h1>
  <p class="lede">Image generation is a workflow: prompt → iterate → constrain → refine. This tutorial gives you a practical map of common controls and how to use them without guessing.</p>

  <h2>Prereqs</h2>
  <ul>
    <li><a href="?t=ai-101-what-models-do">AI 101: What Models Do</a></li>
  </ul>

  <h2>Core knobs</h2>
  <ul>
    <li><strong>Prompt:</strong> intent, composition, style</li>
    <li><strong>Seed:</strong> repeatability for iteration</li>
    <li><strong>Guidance:</strong> how strongly the model follows the prompt</li>
    <li><strong>Refinement:</strong> inpainting-style fixes and iterative edits</li>
  </ul>

  <h2>Worked example: anchor a baseline</h2>
  <ol>
    <li>Choose a subject (e.g., “desk setup with a teal lamp”).</li>
    <li>Set a fixed seed and 3 prompt variants (composition, style, lighting).</li>
    <li>Pick the best; record seed, prompt, and settings.</li>
    <li>Make one targeted change (lighting only) to see controlled differences.</li>
  </ol>

  <h2>Mini build: repeatable loop</h2>
  <ol>
    <li>Pick one subject and goal (product shot, avatar, illustration).</li>
    <li>Generate 3 baselines with the same seed and small prompt tweaks.</li>
    <li>Select the best baseline; apply two targeted edits (guidance change, negative prompt, or inpaint a small region).</li>
    <li>Document which knob caused which improvement.</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Create a repeatable image iteration loop.</p>
    <p><strong>Do:</strong> Pick a subject (product photo, avatar, illustration). Generate 3 variants, then refine the best one with targeted changes.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>You can reproduce a baseline using a fixed seed (or equivalent repeat setting).</li>
      <li>You make at least 2 targeted edits without changing everything.</li>
      <li>You document what change caused which improvement.</li>
    </ul>
    <p><strong>Verify:</strong> Re-run your baseline and confirm you can reach a similar final result using the same steps.</p>
  </section>

  <h2>Practice lab: ComfyUI baseline + controlled edit</h2>
  <ol>
    <li>Load ComfyUI with an SDXL base checkpoint; import a simple txt2img graph.</li>
    <li>Set seed to 12345, steps 30, CFG 7, resolution 1024x1024; prompt: “product shot of a teal desk lamp on a walnut desk, soft daylight”.</li>
    <li>Generate; export the workflow JSON and the PNG.</li>
    <li>Edit only lighting (e.g., “warm tungsten light”), keep the same seed/settings; compare outputs and note what changed.</li>
  </ol>

  <h2>Practice lab: A1111 with explicit settings</h2>
  <ol>
    <li>Load SDXL or SD1.5; set seed 12345, sampler Euler a, steps 30 (SDXL) or 20 (SD1.5), CFG 7, resolution 1024x1024 (SDXL) or 768x768 (SD1.5).</li>
    <li>Prompt: “product shot of a teal desk lamp on a walnut desk, soft daylight”; negative: “blurry, watermark, text”.</li>
    <li>Generate baseline, save the image and the exact settings.</li>
    <li>Change one knob: either lighting wording or guidance (CFG 6→8). Keep seed fixed; note differences.</li>
  </ol>

  <h2>Assets you can reuse</h2>
  <ul>
    <li><a href="assets/examples/image/seeds.csv">Seeds + prompts table</a> (baseline + lighting tweak)</li>
    <li><a href="assets/examples/image/comfyui-baseline.txt">ComfyUI baseline parameters</a> (txt2img SDXL)</li>
    <li><a href="assets/examples/image/README.md">How to recreate baselines</a></li>
  </ul>

  <h2>Practice lab: Automatic1111 with ControlNet/Inpaint</h2>
  <ol>
    <li>Open A1111, load SDXL or SD 1.5; set seed and steps as above.</li>
    <li>Use ControlNet with a simple line-art conditioning (or depth) to anchor composition; generate 2–3 variants.</li>
    <li>Use Inpaint to fix one small region (e.g., logo area); keep the same seed to preserve the rest.</li>
    <li>Document the exact knobs: sampler, steps, CFG, seed, ControlNet weight.</li>
  </ol>

  <h2>Practice lab: consistency checklist</h2>
  <ol>
    <li>Run the same seed across three prompt tweaks (lighting, background, camera angle) and note which tweak breaks consistency most.</li>
    <li>Record settings in a short log (prompt, seed, sampler, steps, CFG, any ControlNet/LoRA).</li>
  </ol>

  <p><strong>Learn more:</strong> <a href="https://huggingface.co/docs/diffusers">Hugging Face Diffusers docs</a> and <a href="https://github.com/comfyanonymous/ComfyUI">ComfyUI</a> for reproducible flows; <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Automatic1111</a> for ControlNet/Inpaint.</p>

  <h2>Start → Run → Verify checklist</h2>
  <ul>
    <li>Start: baseline seed and settings captured (seed, sampler, steps, CFG, resolution, prompt, negative).</li>
    <li>Run: generate baseline + one targeted change (lighting or guidance) with the same seed.</li>
    <li>Verify: changes are attributable to one knob; settings and outputs logged for replay.</li>
  </ul>

  <h2>Recap</h2>
  <ul>
    <li>Fix a baseline with seed + prompt; change one knob at a time.</li>
    <li>Targeted edits beat full re-rolls; document settings so you can repeat.</li>
    <li>Guidance, seed, and small prompt tweaks are your primary control set.</li>
  </ul>

  <h2>Next</h2>
  <p><a href="?t=image-ai-hands-on-controlnet-and-inpainting">Image AI: Hands-on Control + Inpainting</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=video-ai-basics-models-pipelines">Video AI: Models &amp; Pipelines</a></li>
    <li><a href="?t=safety-ethics-practical-ai-use">Safety &amp; Ethics: Practical AI Use</a></li>
  </ul>
</article>
