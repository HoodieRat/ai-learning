<article class="tutorial">
  <h1>RAG: When It Helps and When It Hurts</h1>
  <p class="lede">A practical decision guide: when retrieval boosts accuracy and when it makes things worse.</p>

  <h2>Core concepts</h2>
  <ul>
<li>Context window is not long-term memory; you must re-provide facts.</li>
<li>Chunking and metadata decide retrieval quality.</li>
<li>Grounding + verification reduce hallucinations on long docs.</li>
  </ul>

  <h2>Worked example</h2>
  <ol>
<li>Pick a doc set, chunk with overlap, embed.</li>
<li>Retrieve top-k, then answer with sources inline.</li>
<li>Review misses and adjust chunking or metadata.</li>
  </ol>

  <h2>Mini build</h2>
  <ol>
<li>Build a folder of docs, chunk, embed, and index.</li>
<li>Run a retrieval + answer query and log sources.</li>
<li>Store outputs and misses for later tuning.</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Ask a question, inspect retrieved chunks, and verify grounding.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>You capture parameters and outputs for the run.</li>
      <li>You can repeat the run and get consistent behavior.</li>
      <li>You note at least one risk or failure mode to watch.</li>
    </ul>
  </section>

  <h2>Recap</h2>
  <ul>
<li>Context â‰  memory.</li>
<li>Chunking/metadata drive retrieval.</li>
<li>Verify with sources.</li>
  </ul>

  <h2>Next</h2>
  <p>Pick a related tutorial from the catalog to deepen the skill.</p>
</article>
