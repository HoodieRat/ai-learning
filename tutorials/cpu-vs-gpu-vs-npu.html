<article class="tutorial">
  <h1>Hardware: CPU vs GPU vs NPU</h1>
  <p class="lede">Pick hardware by workload, not hype. This tutorial shows what each chip is good at, how memory shapes everything, and how to choose with real constraints.</p>

  <h2>Prereqs</h2>
  <ul>
    <li><a href="?t=ai-101-what-models-do">AI 101: What Models Do</a></li>
  </ul>

  <h2>Core concepts</h2>
  <ul>
    <li><strong>Parallelism:</strong> GPUs/NPUs win on many small ops; CPUs win on branchy control and I/O.</li>
    <li><strong>Memory fit:</strong> If weights do not fit in fast memory, latency and throughput collapse.</li>
    <li><strong>Bandwidth vs compute:</strong> Chips stall when they cannot feed their ALUs; spec TFLOPs only matter if bandwidth and kernels cooperate.</li>
    <li><strong>Software stack:</strong> Drivers, runtimes, and kernel libraries decide whether theoretical speed shows up.</li>
  </ul>

  <h2>Quick sizing cheat sheet</h2>
  <ul>
    <li><strong>Under ~8B models:</strong> Fit in CPU RAM or 8–12 GB VRAM; good for chat, tools, and lightweight coding.</li>
    <li><strong>10–14B models:</strong> Want 12–20 GB fast memory; midrange GPUs or higher-RAM laptops.</li>
    <li><strong>20B+ models:</strong> Expect 24 GB+ VRAM (or quantize hard); otherwise you will page and stall.</li>
    <li><strong>Image gen (512², SD1/SDXL):</strong> 8–16 GB VRAM; higher if you upscale or use heavy ControlNet stacks.</li>
  </ul>

  <h2>Common bottlenecks to watch</h2>
  <ul>
    <li><strong>VRAM fit:</strong> If the model spills to RAM, throughput can drop 5–30×.</li>
    <li><strong>Bandwidth limits:</strong> PCIe and memory bandwidth throttle big attention layers; TFLOPs alone are misleading.</li>
    <li><strong>Thermals:</strong> Laptops throttle under sustained load; desktops fare better with airflow.</li>
    <li><strong>Kernel coverage:</strong> Some ops (e.g., grouped attention, quantized kernels) may not be optimized on your stack.</li>
  </ul>

  <h2>Worked example: match task to silicon</h2>
  <p>Three common workloads and sensible pairings:</p>
  <ul>
    <li><strong>Chat/inference under 8B:</strong> CPU with optimized kernels or small GPU; favors low-latency, low-thermal setups.</li>
    <li><strong>Image generation:</strong> GPU or NPU with strong tensor cores; needs high bandwidth and enough VRAM for UNet + attention.</li>
    <li><strong>Speech (STT/TTS):</strong> Mixed: GPU for batching, CPU for streaming with lower memory; NPU if the runtime supports the model.</li>
  </ul>

  <h2>Mini build: pick a target device</h2>
  <ol>
    <li>List the top two workloads you actually run (e.g., chat with 4K context, image gen at 512x512).</li>
    <li>Record constraints: available VRAM, RAM, power/thermals (laptop vs desktop), and OS/runtime support.</li>
    <li>Choose one chip per workload (CPU/GPU/NPU) and write one risk each (driver, memory fit, noise/heat).</li>
    <li>Check one benchmark or vendor doc to see if your chip is supported for that workload.</li>
  </ol>

  <div class="learning-callout">
    <p><strong>Reality checks</strong></p>
    <ul>
      <li>Quantization is not free: speed may improve, quality may drop. Test before standardizing.</li>
      <li>NPU wins depend on runtime support. If your model or ops are unsupported, the CPU/GPU may still win.</li>
      <li>Battery and noise matter: a slightly slower CPU run can be acceptable if it avoids fans at max.</li>
    </ul>
  </div>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Map your top AI tasks to hardware and defend the choice.</p>
    <p><strong>Do:</strong> Write down 3 tasks (chat, image gen, transcription). For each, pick CPU/GPU/NPU, list the limiting resource, and note one risk.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>You have a short rationale per task tied to bandwidth/fit, not brand.</li>
      <li>You identify the limiting resource (VRAM, RAM, bandwidth, thermals) for each task.</li>
      <li>You name at least one software risk (driver/runtime support) and one hardware risk (heat/noise/battery).</li>
    </ul>
    <p><strong>Verify:</strong> Find a public benchmark or runtime doc for one task and confirm your choice makes sense.</p>
  </section>

  <h2>Check your understanding</h2>
  <div data-quiz="cpu-vs-gpu-vs-npu.quiz.json"></div>

  <h2>Recap</h2>
  <ul>
    <li>Speed claims only hold if memory fits and bandwidth keeps up.</li>
    <li>Pick hardware per workload profile, not headline TFLOPs.</li>
    <li>Software support can flip the winner; always confirm runtime/driver coverage.</li>
  </ul>

  <h2>Next</h2>
  <p><a href="?t=vram-vs-ram">Hardware: VRAM vs RAM</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=local-ai-ollama-basics">Local AI: Ollama Basics</a></li>
    <li><a href="?t=quantization-explained">Hardware: Quantization Explained</a></li>
  </ul>
</article>
