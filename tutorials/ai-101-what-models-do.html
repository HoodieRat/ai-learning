<article class="tutorial">
  <h1>AI 101: What Models Do</h1>
  <p class="lede">Language models predict the next token, not “the truth.” Knowing that changes how you prompt, how you verify, and how you react to confident but wrong answers.</p>

  <div class="learning-callout">
    <p class="pill">Mini roadmap</p>
    <ul class="checklist">
      <li>Why predicting the next token creates fluent but brittle outputs.</li>
      <li>How hallucinations emerge from missing grounding.</li>
      <li>Simple prompt patterns that make verification easier.</li>
    </ul>
  </div>

  <h2>Prereqs</h2>
  <p>None. This is the mental model that anchors the rest of the curriculum.</p>

  <h2>Core idea: prediction over truth</h2>
  <p>A base model is trained to continue text. It is rewarded for producing the most plausible next token given the context it has seen, not for being factually correct. This is why models can be simultaneously <em>fluent</em> and <em>wrong</em>.</p>

  <ul>
    <li><strong>What the model optimizes:</strong> likelihood of the next token.</li>
    <li><strong>What it does not optimize by default:</strong> correctness, citations, or honesty.</li>
    <li><strong>Why you care:</strong> your prompts must supply structure and verification hooks.</li>
  </ul>

  <h2>How that shows up in practice</h2>
  <div class="practice-lab">
    <p><strong>Pattern 1: Fluent but ungrounded.</strong> The model fills gaps with plausible details when facts are missing.</p>
    <p><strong>Pattern 2: Overconfident style.</strong> The model mirrors the certainty in your prompt unless you explicitly ask for uncertainty.</p>
    <p><strong>Pattern 3: Retrieval gap.</strong> If no source of truth is provided (docs, data, retrieved chunks), it will invent.</p>
  </div>

  <h2>Settings that change behavior</h2>
  <ul>
    <li><strong>Temperature / top_p:</strong> higher values increase variance; lower values make outputs repeatable. Start at 0.2–0.4 for accuracy work.</li>
    <li><strong>Max tokens:</strong> short caps reduce rambling but can truncate. Match to your schema length.</li>
    <li><strong>Stop sequences:</strong> force the model to halt before it wanders; add "\n\n" or closing braces for JSON tasks.</li>
    <li><strong>System vs user:</strong> keep governance in system (“you are cautious”) and task details in user. Avoid hiding constraints in examples.</li>
    <li><strong>Sampling vs greedy:</strong> for safety/consistency, try deterministic ($P(t_i\mid t_{<i})$ with low temperature) plus verification.</li>
  </ul>

  <div class="learning-callout">
    <p><strong>Mini math</strong></p>
    <p>When you lower temperature toward 0, decoding approaches greedy: it picks the argmax of $P(t_i \mid t_{<i})$ at each step. Higher temperature reweights the distribution and raises entropy, which raises variance and hallucination risk.</p>
  </div>

  <h2>Make answers easier to verify</h2>
  <p>Use prompts that separate facts from speculation and surface assumptions.</p>
  <ul>
    <li>Ask for a short answer <em>plus</em> a list of assumptions.</li>
    <li>Request 2–3 ways the answer could be wrong or could change with new info.</li>
    <li>Ask for at least one verifiable claim (something you can check in docs).</li>
    <li>Keep style concise; verbosity hides errors.</li>
  </ul>

  <h2>Grounding options</h2>
  <ul>
    <li><strong>Retrieval (RAG):</strong> attach sources; ask the model to cite chunk IDs. If sources are empty, the model should say "no evidence".</li>
    <li><strong>Structured inputs:</strong> pass tables/JSON where possible; schemas reduce speculation.</li>
    <li><strong>Tool calls:</strong> when a fact can be fetched or computed, have the model call the tool instead of guessing.</li>
  </ul>

  <h2>Model fit</h2>
  <ul>
    <li><strong>Instruct vs base:</strong> prefer instruct-tuned models for chat; base models hallucinate more because they lack the safety layer.</li>
    <li><strong>Domain-tuned:</strong> for code, use a code model; for long docs, choose models with long context and higher recall.</li>
    <li><strong>Latency vs quality:</strong> if you lower temp and add verification, smaller models can work for draft iterations.</li>
  </ul>

  <h2>Examples you can copy</h2>
  <div class="learning-callout">
    <p><strong>Verify-first prompt</strong></p>
    <p>
      “You are a careful assistant. Answer in three parts: (1) a concise answer, (2) assumptions you made, (3) 2–3 ways this could be wrong or would change with new info. Mark any claim that should be verified in docs.”
    </p>
    <p><strong>When to use:</strong> anytime the stakes are non-trivial (policy, customer comms, product guidance).</p>
  </div>

  <h2>Quick checks (do these now)</h2>
  <ol>
    <li>Run a prompt that asks for “answer + assumptions + ways to be wrong” on a topic you know.</li>
    <li>Highlight one assumption and note a source you would check.</li>
    <li>Compare with a direct, unstructured prompt—the difference is the value of structure.</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab: Build a verify-first prompt</h2>
    <p><strong>Task:</strong> Create a verify-first prompt for a topic you know well (e.g., a framework you use).</p>
    <p><strong>Steps:</strong></p>
    <ol>
      <li>Ask for the answer, assumptions, and “how this could be wrong.”</li>
      <li>Add a request for one claim that you can check in public docs.</li>
      <li>Paste the output. Circle the assumptions; underline the verifiable claim.</li>
      <li>Open an official doc and verify one claim. Note any mismatch.</li>
    </ol>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>The response clearly separates answer vs assumptions.</li>
      <li>There are at least 2 stated failure modes.</li>
      <li>You verify one claim using a free source (docs, spec, reputable reference).</li>
    </ul>
  </section>

  <h2>Failure modes to watch</h2>
  <ul>
    <li><strong>Confident uncertainty:</strong> fluent answers with no grounding.</li>
    <li><strong>Citation theater:</strong> references that don’t actually support the claim.</li>
    <li><strong>Hidden assumptions:</strong> missing constraints that change the conclusion.</li>
  </ul>

  <h2>Practice lab: “answer + assumptions + risks”</h2>
  <ol>
    <li>Prompt a local model: <code>ollama run llama3:8b "Answer in three parts: (1) short answer, (2) assumptions, (3) 2 risks if assumptions fail. Topic: &lt;your topic&gt;. Temperature=0.2."</code></li>
    <li>Highlight one assumption and verify it with a real doc or link; note if it was wrong.</li>
    <li>Rerun at temperature 0.8; compare style and hallucination risk.</li>
  </ol>

  <h2>Practice lab: empty vs grounded context</h2>
  <ol>
    <li>Run the same question twice: once with no context, once with a 2–3 sentence fact block prepended.</li>
    <li>Require the model to label any claim as “supported” or “unsupported.”</li>
    <li>Check: did the grounded run reduce unsupported claims? If not, tighten the prompt.</li>
  </ol>

  <h2>Practice lab: stop sequences and truncation</h2>
  <ol>
    <li>Ask for JSON: <code>{"decisions":[],"risks":[]}</code>. Run with <code>--format json</code> (if your runtime supports) and <code>--num-predict 128</code>; observe if it truncates.</li>
    <li>Add a stop sequence (e.g., <code>"\n\n"</code>) and rerun; confirm the model stops cleanly at the schema boundary.</li>
    <li>Increase <code>--num-predict</code> to 256 and verify it still stops at the stop sequence.</li>
  </ol>

  <h2>Practice lab: compare two sampling settings</h2>
  <ol>
    <li>Prompt: “List 5 risks of shipping code without tests. Return bullet points only.”</li>
    <li>Run once at <code>temperature=0.2</code>, once at <code>temperature=0.9</code>; keep everything else fixed.</li>
    <li>Compare variance: which answers drift off-topic? Keep the low-temp prompt as your baseline for risky tasks.</li>
  </ol>

  <p><strong>Learn more:</strong> <a href="https://platform.openai.com/docs/guides/prompt-engineering">OpenAI prompt engineering guide</a>.</p>

  <h2>Next</h2>
  <p><a href="?t=ai-literacy-first-steps-in-chat">AI Literacy: First Steps in Chat</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=prompting-structure-quickstart">Prompting: Structure Quickstart</a></li>
    <li><a href="?t=safety-ethics-practical-ai-use">Safety &amp; Ethics: Practical AI Use</a></li>
  </ul>
</article>
