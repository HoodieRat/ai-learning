<article class="tutorial">
  <h1>Evaluation: Metrics vs Judges</h1>
  <p class="lede">If you can’t measure it, you can’t improve it. This tutorial explains when to use metrics, human review, or LLM-as-judge — and how evals go wrong.</p>

  <h2>Prereqs</h2>
  <ul>
    <li><a href="?t=ai-101-what-models-do">AI 101: What Models Do</a></li>
    <li><a href="?t=prompting-structure-quickstart">Prompting: Structure Quickstart</a></li>
  </ul>

  <h2>Pick the right tool</h2>
  <ul>
    <li><strong>Metrics:</strong> Fast, consistent, best for structured outputs (JSON, lists, spans).</li>
    <li><strong>Human review:</strong> Best for nuance or stakes; slow and costly.</li>
    <li><strong>Judge models:</strong> Scalable nuance; can be biased or unstable without a good rubric.</li>
  </ul>

  <h2>Worked example: match eval to task</h2>
  <ol>
    <li>Task: Summaries of support tickets.</li>
    <li>Definition of good: includes product, issue, impact, and next step.</li>
    <li>Approach: Judge model with a rubric that scores each element 0–2; spot-check 10% with a human.</li>
    <li>What it misses: Tone or subtle empathy signals; mark as a risk.</li>
  </ol>

  <h2>Mini build: your evaluation choice</h2>
  <ol>
    <li>Pick one workflow (summaries, extraction, RAG answers).</li>
    <li>Write 3–5 bullets for "good looks like" (facts, format, safety).</li>
    <li>Choose metric vs human vs judge; note cost/speed/risk tradeoffs.</li>
    <li>Create 5 test cases and do a quick manual score to see if your definition holds.</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Choose an evaluation approach for a real workflow.</p>
    <p><strong>Do:</strong> Pick one workflow. Decide which of the three approaches fits and write why, including one risk.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>You can define what good means in 3–5 bullets.</li>
      <li>You pick a method and justify tradeoffs (cost, speed, reliability).</li>
      <li>You identify one thing your method might miss.</li>
    </ul>
    <p><strong>Verify:</strong> Create 5 test cases and do a quick manual score to see if your definition holds up.</p>
  </section>

  <h2>Practice lab: metric vs judge side-by-side</h2>
  <ol>
    <li>Task: extract <code>{city, temperature_c, condition}</code> from 10 weather snippets.</li>
    <li>Metric pass: check JSON validity and presence of all keys; count successes (simple script or <code>jq</code>).</li>
    <li>Judge pass: prompt a small model (e.g., <code>ollama run llama3:8b</code>) with a rubric (0/1 for each key correct); record scores.</li>
    <li>Compare time: metrics are fast but miss “wrong-but-present” values; judges catch wrong values but cost more tokens/time.</li>
  </ol>

  <h2>Assets and scripts</h2>
  <ul>
    <li><a href="assets/examples/eval/rubric.md">Rubric template</a></li>
    <li><a href="assets/examples/eval/metric_eval.py">Metric checker (stdin JSON)</a></li>
    <li><a href="assets/examples/eval/judge_eval.py">Judge stub script</a> (replace with your model call)</li>
  </ul>

  <h2>Start → Run → Verify checklist</h2>
  <ul>
    <li>Start: define rubric (3–4 criteria) and collect 10 outputs as JSON.</li>
    <li>Run: pipe outputs to <code>metric_eval.py</code>; run judge script with rubric on the same set.</li>
    <li>Verify: compare passes; note disagreements; update rubric if judges over/under-penalize.</li>
  </ul>

  <h2>Practice lab: rubric for summaries</h2>
  <ol>
    <li>Create 6 short support-ticket summaries (model outputs).</li>
    <li>Write a rubric with three 0–2 criteria: includes product/impact/next-step; tone neutral; no invented promises.</li>
    <li>Run a judge model with that rubric (any chat model) and log scores; spot-check 2 with a human and note disagreements.</li>
    <li>Adjust the rubric if judges over-penalize or miss hallucinations.</li>
  </ol>

  <h2>Practice lab: citation check for RAG answers</h2>
  <ol>
    <li>Collect 5 RAG answers with cited chunk IDs.</li>
    <li>Script: for each answer, verify the cited chunk text actually supports the claim; flag any “citation theater.”</li>
    <li>Report: % answers with supporting citations; if low, tighten retrieval or prompt.</li>
  </ol>

  <p><strong>Learn more:</strong> <a href="https://github.com/openai/evals">OpenAI Evals</a> and <a href="https://python.langchain.com/docs/guides/evaluation">LangChain evals</a> for runnable patterns.</p>

  <h2>Recap</h2>
  <ul>
    <li>Pick eval methods by output type and stakes; mix approaches when needed.</li>
    <li>Write down what good means before picking metrics or judges.</li>
    <li>Every method has blind spots; name them upfront.</li>
  </ul>

  <h2>Next</h2>
  <p><a href="?t=evaluation-hands-on-design-a-rubric-and-judge">Evaluation: Hands-on Rubric + Judge</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=rag-basics-chunking-retrieval">Knowledge &amp; RAG: Chunking + Retrieval</a></li>
    <li><a href="?t=safety-ethics-practical-ai-use">Safety &amp; Ethics: Practical AI Use</a></li>
  </ul>
</article>
