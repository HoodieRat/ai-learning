<article class="tutorial">
  <h1>Compare Two Outputs: Picking the Better Result (and Why)</h1>
  <p class="lede">Learn a quick method to judge outputs for accuracy, constraint-following, and usefulness.</p>

  <h2>Core concepts</h2>
  <ul>
<li>Define criteria before judging outputs.</li>
<li>Pairwise or rubric-based review beats vibes.</li>
<li>Record scores and rationales to track regressions.</li>
  </ul>

  <h2>Worked example</h2>
  <ol>
<li>Create 5â€“10 test cases with clear expected attributes.</li>
<li>Score outputs against a rubric (e.g., correctness, format).</li>
<li>Record failures and propose a prompt/model tweak.</li>
  </ol>

  <h2>Mini build</h2>
  <ol>
<li>Collect examples with expected attributes.</li>
<li>Score with a simple rubric or pairwise vote.</li>
<li>Emit a short report of fails + proposed fixes.</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Score 5 outputs with a rubric and identify top failure modes.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>You capture parameters and outputs for the run.</li>
      <li>You can repeat the run and get consistent behavior.</li>
      <li>You note at least one risk or failure mode to watch.</li>
    </ul>
  </section>

  <h2>Recap</h2>
  <ul>
<li>Define criteria first.</li>
<li>Score with rubrics/pairwise.</li>
<li>Track failures and fixes.</li>
  </ul>

  <h2>Next</h2>
  <p>Pick a related tutorial from the catalog to deepen the skill.</p>
</article>
