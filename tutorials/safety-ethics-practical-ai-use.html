<article class="tutorial">
  <h1>Safety &amp; Ethics: Practical AI Use</h1>
  <p class="lede">Safety isn’t theoretical; it’s operational. This tutorial covers the everyday practices that reduce harm: data boundaries, disclosure, and verification for high-stakes outputs.</p>

  <h2>Prereqs</h2>
  <ul>
    <li><a href="?t=ai-101-what-models-do">AI 101: What Models Do</a></li>
  </ul>

  <h2>Practical rules</h2>
  <ul>
    <li><strong>Data minimization:</strong> don’t paste secrets or personal data unless you must.</li>
    <li><strong>Disclosure:</strong> label AI-assisted content when it matters.</li>
    <li><strong>Verification:</strong> treat outputs as drafts, not facts.</li>
  </ul>

  <h2>Worked example: safe drafting</h2>
  <ol>
    <li>Task: draft a customer email.</li>
    <li>Rules: no account numbers; no unreleased roadmap; disclose “AI-assisted draft.”</li>
    <li>Verification: check tone, facts, and remove any names not provided.</li>
    <li>Outcome: a templated checklist you can reuse for future drafts.</li>
  </ol>

  <h2>Mini build: your team rules</h2>
  <ol>
    <li>List 4 data types to avoid (secrets, PII, contracts, medical).</li>
    <li>Define when disclosure is required and the exact phrasing.</li>
    <li>Write a verification checklist for high-stakes outputs.</li>
    <li>Add one escalation path (who to ask when unsure).</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Create “safe use” rules for your work.</p>
    <p><strong>Do:</strong> Write 8–12 bullet rules for your team covering: data, IP, external sharing, and verification for critical decisions.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>The rules are specific enough to follow day-to-day.</li>
      <li>At least 3 rules are about verification (not just “be careful”).</li>
      <li>You include one escalation path (who to ask when unsure).</li>
    </ul>
    <p><strong>Verify:</strong> Apply the rules to one real task and adjust them based on what was unclear.</p>
  </section>

  <h2>Practice prompts</h2>
  <ul>
    <li>Draft safe-use rules for marketing content: include disclosure wording and a verification checklist for claims.</li>
    <li>Write a prompt that redacts PII from support logs before sending to a model; specify what counts as PII for your team.</li>
    <li>Create an escalation playbook for high-stakes outputs (medical, legal, finance) with one named reviewer and a blocking condition.</li>
  </ul>
  <p><strong>Learn more:</strong> <a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf">NIST AI RMF</a> for risk management principles to adapt locally.</p>

  <h2>Recap</h2>
  <ul>
    <li>Safety is habits: minimize data, disclose when relevant, verify before use.</li>
    <li>Write rules specific enough to follow daily and add an escalation contact.</li>
    <li>Apply rules to a real task to see what is missing.</li>
  </ul>

  <h2>Next</h2>
  <p><a href="?t=safety-ethics-hands-on-policy-and-red-teaming">Safety &amp; Ethics: Hands-on Policy + Red Team</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=coding-ai-pitfalls-overreliance-and-security">Coding AI: Pitfalls</a></li>
    <li><a href="?t=agentic-flows-pitfalls-infinite-loops-and-tool-misuse">Agentic Flows: Pitfalls</a></li>
  </ul>
</article>
