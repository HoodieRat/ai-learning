<article class="tutorial">
  <h1>Knowledge &amp; RAG: Chunking + Retrieval</h1>
  <p class="lede">RAG is a pipeline: you chunk documents, embed them, retrieve relevant pieces, and then answer. Most failures are pipeline failures — not “the model is dumb”.</p>

  <h2>Prereqs</h2>
  <ul>
    <li><a href="?t=ai-101-what-models-do">AI 101: What Models Do</a></li>
  </ul>

  <h2>Setup</h2>
  <ol>
    <li>Install deps (PowerShell): <code>python -m venv .rag-venv; .\.rag-venv\Scripts\Activate.ps1; pip install "langchain&gt;=0.2" "langchain-community&gt;=0.2" chromadb sentence-transformers</code></li>
    <li>Optional OpenAI fallback: <code>pip install openai langchain-openai</code> then set <code>USE_OPENAI=1</code> and <code>OPENAI_API_KEY</code>.</li>
    <li>Assets: sample corpus, QA, and scripts live in <code>assets/examples/rag/</code> (included in repo).</li>
  </ol>

  <h2>Core concepts</h2>
  <ul>
    <li><strong>Chunking:</strong> Split docs into retrievable units with headers that carry context.</li>
    <li><strong>Retrieval:</strong> Select the few chunks that actually support the answer.</li>
    <li><strong>Grounding:</strong> Responses should cite retrieved text; bad chunks cause bad answers.</li>
  </ul>

  <h2>Run the sample RAG</h2>
  <ol>
    <li>Activate the env: <code>.\.rag-venv\Scripts\Activate.ps1</code></li>
    <li>Run: <code>python assets/examples/rag/rag_quickstart.py</code></li>
    <li>Observe per-question outputs and sources (files from the sample corpus).</li>
  </ol>

  <h2>What the sample does</h2>
  <ul>
    <li>Loads 3 markdown docs (PTO, expenses, remote work) and chunks at ~500 tokens with 50 overlap.</li>
    <li>Embeds with <code>all-MiniLM-L6-v2</code> into a persisted Chroma DB at <code>assets/examples/rag/.chroma</code>.</li>
    <li>Queries via <code>ChatOllama</code> (<code>llama3:8b</code> default) or OpenAI if <code>USE_OPENAI=1</code>.</li>
    <li>Enforces citations: prompt demands <code>[file:section]</code>; answers include retrieved sources.</li>
  </ul>

  <h2>Worked example: from doc to chunk</h2>
  <ol>
    <li>Take a policy PDF with sections and dates.</li>
    <li>Chunk by headings; keep metadata: title, section heading, page, date.</li>
    <li>Write two target questions (e.g., "What is the PTO cap?" and "Who approves exceptions?").</li>
    <li>Verify that each question has at least one chunk that clearly answers it.</li>
  </ol>

  <h2>Verify on the sample QA set</h2>
  <ol>
    <li>Run: <code>python assets/examples/rag/rag_eval.py</code></li>
    <li>Check pass/fail per question; summary accuracy should be printed (expect ~100% on the tiny set).</li>
    <li>Open <code>assets/examples/rag/qa.csv</code> to see the questions, expected answers, and citations.</li>
  </ol>

  <h2>Mini build: a first chunking plan</h2>
  <ol>
    <li>Pick one document type (policies, docs, meeting notes).</li>
    <li>Choose chunk size (by heading or ~400–800 tokens) and metadata (title, section, date, source).</li>
    <li>Define two question types you must answer: fact lookup and multi-sentence explanation.</li>
    <li>Spot-check 3 random chunks to confirm they include enough context without being too long.</li>
  </ol>

  <section class="practice-lab">
    <h2>Practice Lab</h2>
    <p><strong>Task:</strong> Design a chunking plan for one document set.</p>
    <p><strong>Do:</strong> Choose a doc type. Decide chunk size and the metadata you will keep (title, section, date). Write two questions your system must answer.</p>
    <p><strong>Success criteria:</strong></p>
    <ul>
      <li>Your plan includes metadata that improves retrieval (at least title and section).</li>
      <li>You can name two question types the system should answer and which chunks cover them.</li>
      <li>You list two ways chunking can fail (too small, too big, missing headers).</li>
    </ul>
    <p><strong>Verify:</strong> Manually pick three questions and confirm you can find supporting text in your chosen chunks.</p>
  </section>

  <div class="learning-callout">
    <p><strong>Adapt the sample to your docs</strong></p>
    <ol>
      <li>Replace the markdown files in <code>assets/examples/rag/corpus</code> with your own.</li>
      <li>Adjust <code>chunk_size</code> or metadata in <code>rag_quickstart.py</code> to fit your structure.</li>
      <li>Update <code>qa.csv</code> with 6–10 of your questions and rerun <code>rag_eval.py</code>.</li>
    </ol>
  </div>

  <h2>Start → Run → Verify checklist</h2>
  <ul>
    <li>Start: deps installed; sample corpus present; environment activated.</li>
    <li>Run: <code>rag_quickstart.py</code> produces answers with citations; <code>.chroma</code> folder created.</li>
    <li>Verify: <code>rag_eval.py</code> shows high accuracy on <code>qa.csv</code>; citations point to real chunks.</li>
  </ul>

  <h2>Practice lab: build a tiny RAG with LangChain + Chroma</h2>
  <ol>
    <li>Install: <code>pip install langchain chromadb sentence-transformers</code> and download a small corpus (3–5 Markdown or PDF pages).</li>
    <li>Chunk: use a 500–800 token splitter with overlap 50; store metadata: title, section, page, date.</li>
    <li>Embed: use <code>sentence-transformers/all-MiniLM-L6-v2</code> or <code>bge-small-en</code>; write chunks + metadata into Chroma.</li>
    <li>Query: ask two questions (fact lookup, explanation). Retrieve top 3 chunks and return chunk IDs alongside the answer.</li>
    <li>Fail-safe: if no chunk score &gt; threshold, return “no supporting chunk.”</li>
  </ol>

  <h2>Practice lab: quick eval</h2>
  <ol>
    <li>Create 6 QA pairs (3 known-answer, 3 tricky). For each, record if the top chunk actually contains the answer.</li>
    <li>Track accuracy (% of answers with a supporting chunk) and note misses; adjust chunk size or metadata if accuracy &lt; 70%.</li>
  </ol>

  <h2>Practice lab: prompt with citations</h2>
  <ol>
    <li>In your answer prompt, require citing chunk IDs and stating “no evidence” if no chunk supports a claim.</li>
    <li>Run the two main questions again and verify the cited chunk IDs match the retrieved set.</li>
  </ol>

  <p><strong>Learn more:</strong> <a href="https://python.langchain.com/docs/use_cases/question_answering/">LangChain QA guide</a> and <a href="https://docs.trychroma.com/">Chroma docs</a> for retrieval patterns.</p>

  <h2>Recap</h2>
  <ul>
    <li>Chunk with structure and metadata so retrieval stays grounded.</li>
    <li>Design around the questions you need to answer, not generic embeddings.</li>
    <li>Manually verify early that every critical question has a supporting chunk.</li>
  </ul>

  <h2>Next</h2>
  <p><a href="?t=rag-hands-on-chunking-and-retrieval-tuning">Knowledge &amp; RAG: Hands-on Tuning</a></p>

  <h2>Related</h2>
  <ul>
    <li><a href="?t=evaluation-basics-metrics-vs-judges">Evaluation: Metrics vs Judges</a></li>
    <li><a href="?t=safety-ethics-practical-ai-use">Safety &amp; Ethics: Practical AI Use</a></li>
  </ul>
</article>
